<!DOCTYPE html>
<html>
   <head>
   <style type="text/css" media="all">

	body{background: #466368;
		background: -webkit-linear-gradient(#999999, #990000);
		background:    -moz-linear-gradient(#999999, #990000);
		background:         linear-gradient(#999999, #990000);}
	img{}
	h2{ color:white; font-size:30px; text-align:center;}
	ol{ color:white; font-family:Times New Roman; font-style:normal; font-size:20pt; }
	table{ color:white; font-family:Arial; font-style:normal; font-size:13pt }
	p{ color:white; text-align:left;}
	a{color:white;}
	
	
	</style>
      <title>    Intro to Parallel Programming       </title>
   </head>
   <body><meta name = "keywords" content = "computer, science, intro, 100, CSC, HTML">
   			<div id="image" style="display:inline;">
			<center><img style="height:147px; width:486px" src="StMartinlogoTP.png"/>
			</div>
			
			<h2>  Welcome to Parallel Programming Class! </h2>
			<h2>  Chapter 11 - MPI Collective Communication </h2></font>
			<center><p>
<p>MPI_Bcast

 
<p>If comm is an intracommunicator, MPI_BCAST broadcasts a message from the process with rank root to all processes of the group, itself included. It is called by all members of the group using the same arguments for comm and root. On return, the content of root’s buﬀer is copied to all other processes. 

<p>General, derived datatypes are allowed for datatype. The type signature of count, datatype on any process must be equal to the type signature of count, datatype at the root. This implies that the amount of data sent must be equal to the amount received, pairwise between each process and the root. MPI_BCAST and all other data-movement collective routines make this restriction. Distinct type maps between sender and receiver are still allowed.


<p><p>MPI_Reduce
<p>The reduction operation can be either one of a predeﬁned list of operations, or a user-deﬁned operation. The global reduction functions come in several ﬂavors: a reduce that returns the result of the reduction to one member of a group, an all-reduce that returns this result to all members of a group, and two scan (parallel preﬁx) operations. In addition, a reduce-scatter operation combines the functionality of a reduce and of a scatter operation.
 


<p><p>MPI_Allreduce	
<p>MPI includes a variant of the reduce operations where the result is returned to all processes in a group. MPI requires that all processes from the same group participating in these operations receive identical results.
 


<p><p>MPI_Barrier
<p>If comm is an intracommunicator, MPI_BARRIER blocks the caller until all group members have called it. The call returns at any process only after all group members have entered the call. If comm is an intercommunicator, MPI_BARRIER involves two groups. The call returns at processes in one group (group A) of the intercommunicator only after all members of the other group (group B) have entered the call (and vice versa). A process may return from the call before all processes in its own group have entered the call.
 


<p><p>MPI_Scatter
<p>MPI_SCATTER is the inverse operation to MPI_GATHER. If comm is an intracommunicator, the outcome is as if the root executed n send operations, and each process executed a receive.
 


<p>(https://www.mpi-forum.org/docs/mpi-3.0/mpi30-report.pdf) 

			</center></p>

			
			<p><a href = "https://moodle.stmartin.edu/">Link to Course Moodle Page</a>


   </body>
</html>